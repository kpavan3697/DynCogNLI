"""
llm_responder.py

This module provides the LLMResponder class, which generates conversational responses using an OpenAI-compatible language model.
It takes user queries and persona insights (from the GNN) and produces empathetic, context-aware replies for the DynCogNLI system.
"""
import os
from openai import OpenAI
import streamlit as st

class LLMResponder:
    """
    Generates conversational responses using an OpenAI (or compatible) LLM.
    """
    def __init__(self, model_name="gpt-3.5-turbo"):
        self.model_name = model_name
        self.client = None
        
        # Prioritize Streamlit secrets, then environment variables
        api_key = st.secrets.get("OPENAI_API_KEY") or os.getenv("OPENAI_API_KEY")

        if not api_key:
            raise ValueError("OPENAI_API_KEY not found in Streamlit secrets or environment variables.")
        
        try:
            self.client = OpenAI(api_key=api_key)
            print(f"LLMResponder initialized with model: {model_name}")
        except Exception as e:
            raise RuntimeError(f"Failed to initialize OpenAI client: {e}. Check your API key.")


    def generate_response(self, user_query: str, persona_insight: str) -> str:
        """
        Generates a conversational response based on the user query and persona insight.

        Args:
            user_query (str): The original user query.
            persona_insight (str): The persona insight generated by the GNN.

        Returns:
            str: A natural language conversational response.
        """
        if not self.client:
            return "LLM service is not available due to an initialization error."

        prompt = (
            f"User Query: '{user_query}'\n\n"
            f"Detailed Persona Analysis: {persona_insight}\n\n"
            "Based on the user query and the detailed persona analysis, generate a helpful, empathetic, "
            "and concise conversational response. Keep it under 150 words. "
            "Incorporate the tone, urgency, empathy, and practical needs suggested by the persona analysis. "
            "Address the user directly."
        )

        try:
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": "You are a highly empathetic and intelligent AI assistant designed to understand user needs and provide helpful, compassionate responses."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=200, # Increased max_tokens for more flexibility
                temperature=0.7 # Adjust creativity
            )
            return response.choices[0].message.content
        except Exception as e:
            print(f"Error generating LLM response: {e}")
            return f"I apologize, but I'm having trouble generating a conversational response right now. (Error: {e})"