"""
llm_responder.py

This module provides the LLMResponder class, which generates conversational responses using an
OpenAI-compatible language model. It takes user queries and persona insights (from the GNN)
and produces empathetic, context-aware replies for the DynCogNLI system.

The class handles API key management, client initialization, and prompt engineering to
ensure the LLM's response is tailored to the user's situation as identified by the GNN.
"""
import os
from openai import OpenAI
import streamlit as st
from typing import Optional

class LLMResponder:
    """
    Generates conversational responses using an OpenAI (or compatible) LLM.

    This class encapsulates the logic for interacting with a large language model. It is designed
    to be a key component of the DynCogNLI system, translating the deep graph-based insights
    into a natural, human-like response.
    """
    
    def __init__(self, model_name: str = "gpt-3.5-turbo"):
        """
        Initializes the LLMResponder, setting up the API client.

        Args:
            model_name (str): The name of the LLM model to be used for generating responses.
                              Defaults to "gpt-3.5-turbo".
        
        Raises:
            ValueError: If the OpenAI API key is not found in either Streamlit secrets or
                        environment variables.
            RuntimeError: If the OpenAI client fails to initialize, likely due to an invalid
                          API key.
        """
        self.model_name = model_name
        self.client: Optional[OpenAI] = None
        
        # Prioritize Streamlit secrets, then fall back to environment variables.
        # This makes the application deployment more flexible, working both locally
        # and on Streamlit Cloud.
        api_key = st.secrets.get("OPENAI_API_KEY") or os.getenv("OPENAI_API_KEY")

        if not api_key:
            raise ValueError("OPENAI_API_KEY not found in Streamlit secrets or environment variables.")
        
        try:
            # Initialize the OpenAI client with the retrieved API key.
            self.client = OpenAI(api_key=api_key)
            print(f"LLMResponder initialized with model: {model_name}")
        except Exception as e:
            # Handle potential initialization errors, such as an invalid API key.
            raise RuntimeError(f"Failed to initialize OpenAI client: {e}. Check your API key.")


    def generate_response(self, user_query: str, persona_insight: str) -> str:
        """
        Generates a conversational response based on the user query and persona insight.

        The method constructs a detailed prompt that combines the user's original input with the
        persona analysis from the GNN. This allows the LLM to generate a response that is not
        only relevant to the query but also empathetic and tailored to the user's emotional
        state and needs.

        Args:
            user_query (str): The original user query.
            persona_insight (str): The persona insight generated by the GNN (e.g., "The user is a
                                   parent experiencing a moderate level of stress and needs
                                   a concise, practical solution to a common household problem.").

        Returns:
            str: A natural language conversational response.
        """
        if not self.client:
            return "LLM service is not available due to an initialization error."

        # Craft the system prompt to guide the LLM's behavior.
        # The prompt combines the original user query and the persona insight from the GNN.
        # This is a key step in prompt engineering for the DynCogNLI system.
        prompt = (
            f"User Query: '{user_query}'\n\n"
            f"Detailed Persona Analysis: {persona_insight}\n\n"
            "Based on the user query and the detailed persona analysis, generate a helpful, empathetic, "
            "and concise conversational response. Keep it under 150 words. "
            "Incorporate the tone, urgency, empathy, and practical needs suggested by the persona analysis. "
            "Address the user directly."
        )

        try:
            # Call the OpenAI chat completions API to get a response.
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    # The system message defines the AI's role and tone.
                    {"role": "system", "content": "You are a highly empathetic and intelligent AI assistant designed to understand user needs and provide helpful, compassionate responses."},
                    # The user message contains the core prompt with the persona context.
                    {"role": "user", "content": prompt}
                ],
                max_tokens=200,  # Increased max_tokens for more flexibility in the response.
                temperature=0.7  # Adjust creativity level; 0.7 is a good balance for helpfulness.
            )
            return response.choices[0].message.content
        except Exception as e:
            # Catch any potential errors during the API call, such as network issues or
            # model-specific errors.
            print(f"Error generating LLM response: {e}")
            return f"I apologize, but I'm having trouble generating a conversational response right now. (Error: {e})"
